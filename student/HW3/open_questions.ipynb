{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c7fcefa2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Assignment 3: Sentiment Classification Reflection\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    self-contained: true\n",
    "    number-sections: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65134d1f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "print(\"I'm running\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "01216b3c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# display images from script\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pathlib\n",
    "\n",
    "folder_path = pathlib.Path(__file__).parent.resolve() / \"outputs\"\n",
    "image_files = sorted([f for f in os.listdir(folder_path) if f.endswith(\".png\")])\n",
    "\n",
    "n_images = len(image_files)\n",
    "cols = 3\n",
    "rows = math.ceil(n_images / cols)\n",
    "\n",
    "plt.figure(figsize=(15, 5 * rows))\n",
    "\n",
    "for i, img_name in enumerate(image_files):\n",
    "    img_path = os.path.join(folder_path, img_name)\n",
    "    img = mpimg.imread(img_path)\n",
    "    \n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(img_name)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e1bed8",
   "metadata": {},
   "source": [
    "## Open-Ended Reflection Questions\n",
    "## 1. Train Dynamics\n",
    "The LSTM model showed strong signs of overfitting, as the training loss went to zero and F1 went to one. The MLP model showed some signs of overfitting, but had a smaller train-validation gap and more stable validation curves. Changes we could make are early stopping, increased dropout, or weight decay. <br/>\n",
    "Using class weights improved macro-F1 and minority class recall for both models, but in the LSTM it probably amplified gradient magnitudes and increased training instability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1058fa0",
   "metadata": {},
   "source": [
    "## 2. Model Performance and Error Analysis\n",
    "The LSTM generalized better to the test set, as it achieved higher validation accuracy (around 76–78%) and higher macro-F1 (around 0.73–0.75) compared to the MLP (around 70% accuracy and 0.67 macro-F1). Even though it was overfitting more during training, the LSTM still captured richer sequential information and achieved better overall generalization performance. <br/>\n",
    "\n",
    "The Neutral class was most frequently misclassified in both models. The confusion is seen both between Neutral and Positive and between Neutral and Negative examples. This is likely because of the semantic ambiguity, where mildly positive or negative sentences could be similar to neutral statements in the embedding space. Also, Neutral was the largest class, so that likely contributes to the absolute number of misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd269dcf",
   "metadata": {},
   "source": [
    "## 3. Cross-Model Comparison\n",
    "The mean-pooled FastText embeddings get rid of word order and contextual dependencies. This prevents the MLP from capturing sentence-level structure that sequence-based models can learn.\n",
    "The LSTM’s way of sequential processing allows it to model word order and compositional structure, which is why it has an advantage over MLP. \n",
    "Fine-tuned BERT outperformed classical neural baselines because of large-scale pretraining and contextual self-attention. The difference for BERT is that it dynamically adjusts word representations based on sentence context, allowing it to better distinguish sentiment differences and- reduce class confusion.\n",
    "Looking at the performance, we can rank the six models in descending order: GPT, BERT, GRU, LSTM, RNN, MLP, with GRU and LSTM having a rather narrow gap. To explain this, we can argue that MLP is at the bottom because of its mean-pooled embeddings, which does not take into account word order or context. Then, the sequence-based models(RNN, LSTM, GRU) have token order and long-range dependencies, which improves performance. Amongst them, the gated models(LSTM, GRU) have better gradient flow. Then, BERT's pretraining is one more step over LSTM. Finally, GPT, like BERT, is a pretrained transformer, but is larger, leading to best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Use Disclosure (Required)\n",
    "\n",
    "If you used any AI-enabled tools (e.g., ChatGPT, GitHub Copilot, Claude, or other LLM assistants) while working on this assignment, you must disclose that use here. The goal is transparency-not punishment.\n",
    "\n",
    "In your disclosure, briefly include:\n",
    "- **Tool(s) used:** (name + version if known)\n",
    "- **How you used them:** (e.g., concept explanation, debugging, drafting code, rewriting text)\n",
    "- **What you verified yourself:** (e.g., reran the notebook, checked outputs/plots, checked shapes, read documentation)\n",
    "- **What you did *not* use AI for (if applicable):** (optional)\n",
    "\n",
    "You are responsible for the correctness of your submission, even if AI suggested code or explanations.\n",
    "\n",
    "#### <font color=\"red\">Write your disclosure here.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57babbee",
   "metadata": {},
   "source": [
    "Used ChatGPT to debug and provide a starting point for pytorch code. I then read, understood, and edited the code it gave me using documentations. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
